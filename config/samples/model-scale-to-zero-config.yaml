# ConfigMap for per-model scale-to-zero configuration
#
# This ConfigMap defines scale-to-zero behavior for specific models using a prefixed-key
# format that supports any model ID without character restrictions or collision risks,
# while allowing independent editing of each model's configuration.
#
# Key Format:
#   - Global defaults: "__defaults__" (special key)
#   - Per-model configs: "model.<safe-key>" where safe-key uses dots instead of slashes/colons
#   - Each value is YAML containing the namespace, original modelID, and configuration
#
# Benefits of this approach:
#   Independently editable - Change one model without touching others
#   Easy kubectl operations - Add/update/remove individual models
#   Better Git diffs - Only changed models show in version control
#   No collision risk - Original modelID preserved in YAML value
#   Semi-human-readable - Keys like "model.meta.llama-3.1-8b" are recognizable
#   Namespace-aware - Same modelID can have different configs in different namespaces
#
# Configuration fields:
#   - namespace (string): Kubernetes namespace (omit for global/all namespaces)
#   - modelID (string): Original model ID with any characters (/, :, @, etc.)
#   - enableScaleToZero (boolean): Enables scale-to-zero for this model
#   - retentionPeriod (string): Duration after last request before scaling to zero
#                                (e.g., "5m", "1h", "30s"). Optional, defaults to 10 minutes.
#
# Configuration priority (highest to lowest):
#   1. Namespace-specific model configuration (namespace + modelID match)
#   2. Global model configuration (modelID match, no namespace specified)
#   3. Global defaults in this ConfigMap (key: "__defaults__")
#   4. WVA_SCALE_TO_ZERO environment variable
#   5. System default (disabled, 10-minute retention)
#
# Easy Operations:
#   # Update a single model (use YAML format):
#   kubectl patch configmap model-scale-to-zero-config -n workload-variant-autoscaler-system \
#     --type merge -p $'{"data":{"model.meta.llama-3.1-8b":"modelID: \\"meta/llama-3.1-8b\\"\\nretentionPeriod: \\"3m\\"\\n"}}'
#
#   # Or use kubectl edit for easier YAML editing:
#   kubectl edit configmap model-scale-to-zero-config -n workload-variant-autoscaler-system
#
#   # Remove a model:
#   kubectl patch configmap model-scale-to-zero-config -n workload-variant-autoscaler-system \
#     --type json -p '[{"op":"remove","path":"/data/model.meta.llama-3.1-8b"}]'

apiVersion: v1
kind: ConfigMap
metadata:
  name: model-scale-to-zero-config
  namespace: workload-variant-autoscaler-system
data:
  # Global defaults for all models (special key)
  __defaults__: |
    enableScaleToZero: true
    retentionPeriod: "15m"

  # Example 1: NAMESPACE-SPECIFIC CONFIGURATION (Production)
  # Same model ID in production namespace with strict settings
  # Result: Production instance has scale-to-zero DISABLED (always available)
  model.prod.meta.llama-3.1-8b: |
    namespace: "production"
    modelID: "meta/llama-3.1-8b"
    enableScaleToZero: false

  # Example 2: NAMESPACE-SPECIFIC CONFIGURATION (Development)
  # Same model ID in development namespace with relaxed settings
  # Result: Development instance can scale to zero after 5 minutes
  model.dev.meta.llama-3.1-8b: |
    namespace: "development"
    modelID: "meta/llama-3.1-8b"
    enableScaleToZero: true
    retentionPeriod: "5m"

  # Example 3: GLOBAL MODEL CONFIGURATION
  # No namespace specified - applies to all namespaces (unless overridden)
  # Result: Any namespace using this model will scale to zero after 15 minutes
  model.meta.llama-3.1-70b: |
    modelID: "meta/llama-3.1-70b"
    enableScaleToZero: true
    retentionPeriod: "15m"

  # Example 4: NAMESPACE-SPECIFIC OVERRIDE OF GLOBAL
  # This overrides the global config for meta/llama-3.1-70b in staging
  model.staging.meta.llama-3.1-70b: |
    namespace: "staging"
    modelID: "meta/llama-3.1-70b"
    enableScaleToZero: false  # Staging needs to stay up

  # Example 5: PARTIAL OVERRIDE (inherits from defaults)
  # Only override retentionPeriod, inherit enableScaleToZero from defaults
  # Result: scale-to-zero ENABLED (from defaults) with 3-minute retention
  model.mistralai.Mistral-7B-v0.1: |
    modelID: "mistralai/Mistral-7B-v0.1"
    retentionPeriod: "3m"

  # Example 6: MODEL ID WITH COLON (vllm prefix) IN SPECIFIC NAMESPACE
  # Demonstrates namespace-awareness with complex model IDs
  model.test.vllm.meta.llama-3.1-8b: |
    namespace: "test"
    modelID: "vllm:meta/llama-3.1-8b"
    enableScaleToZero: true
    retentionPeriod: "1m"

  # Example 7: MULTI-NAMESPACE DEPLOYMENT
  # Same base model in three different namespaces with different behaviors:
  # - production: Always on (no scale-to-zero)
  # - staging: Scale to zero after 30 minutes
  # - development: Scale to zero after 5 minutes
  model.prod.meta.llama-2-7b: |
    namespace: "production"
    modelID: "meta/llama-2-7b"
    enableScaleToZero: false

  model.staging.meta.llama-2-7b: |
    namespace: "staging"
    modelID: "meta/llama-2-7b"
    enableScaleToZero: true
    retentionPeriod: "30m"

  model.dev.meta.llama-2-7b: |
    namespace: "development"
    modelID: "meta/llama-2-7b"
    enableScaleToZero: true
    retentionPeriod: "5m"

# Note: Models not explicitly listed (e.g., "meta/llama-3.1-13b") will inherit
# settings from "__defaults__". If "__defaults__" is not specified, models fall back to
# the WVA_SCALE_TO_ZERO environment variable.
