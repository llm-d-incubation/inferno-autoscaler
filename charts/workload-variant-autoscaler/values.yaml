wva:
  enabled: true

  image:
    repository: ghcr.io/llm-d/workload-variant-autoscaler
    tag: latest
  imagePullPolicy: Always

  metrics:
    enabled: true
    port: 8443
    secure: true
  
  reconcileInterval: 60s
    
  prometheus:
    monitoringNamespace: openshift-user-workload-monitoring
    baseURL: "https://thanos-querier.openshift-monitoring.svc.cluster.local:9091"
    # Security configuration
    tls:
      # Note: insecureSkipVerify defaults to true for backward compatibility
      # Override to false in production deployments for strict TLS verification
      caCertPath: "/etc/ssl/certs/prometheus-ca.crt"
    # caCert: |  # Uncomment and provide your CA certificate
    #   -----BEGIN CERTIFICATE-----
    #   YOUR_CA_CERTIFICATE_HERE
    #   -----END CERTIFICATE-----
llmd:
  namespace: llm-d-inference-scheduler
  modelName: ms-inference-scheduling-llm-d-modelservice
  modelID: "unsloth/Meta-Llama-3.1-8B"

va:
  enabled: true
  # Variant identification
  variantID: "unsloth/Meta-Llama-3.1-8B-H100-1"

  # Accelerator configuration
  accelerator: H100
  acceleratorCount: 1

  # SLO configuration (used in service class ConfigMap)
  sloTpot: 10
  sloTtft: 1000

  # Replica bounds (optional)
  minReplicas: 0
  maxReplicas: 10

  # Variant cost (optional, defaults to "10")
  # variantCost: "40.00"

  # Performance parameters
  maxBatchSize: 256
  perfParms:
    decodeParms:
      alpha: "20.58"
      beta: "0.41"
    prefillParms:
      gamma: "5.2"
      delta: "0.1"

hpa:
  enabled: true
  maxReplicas: 10
  targetAverageValue: "1"

vllmService:
  enabled: true
  nodePort: 30000
  interval: 15s
  scheme: http  # vLLM emulator runs on HTTP
