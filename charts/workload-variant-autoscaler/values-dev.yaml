# Development values for workload-variant-autoscaler
# This file contains development-specific configurations with relaxed security settings

wva:
  enabled: true
  baseName: inference-scheduling
  replicaCount: 1
  image:
    repository: ghcr.io/llm-d/workload-variant-autoscaler
    tag: latest
  imagePullPolicy: Always

  reconcileInterval: 30s

  modelName: ms-inference-scheduling-llm-d-modelservice

  metrics:
    enabled: true
    port: 8443
    secure: true
  
  # Development logging configuration
  logging:
    level: debug  # Development: debug, Production: info
  prometheus:
    monitoringNamespace: openshift-user-workload-monitoring
    baseURL: "https://thanos-querier.openshift-monitoring.svc.cluster.local:9091"
    # Development security configuration (relaxed for easier development)
    tls:
      insecureSkipVerify: true   # Development: true, Production: false
      caCertPath: "/etc/ssl/certs/prometheus-ca.crt"
    caCert: |  # Uncomment and provide your CA certificate
      -----BEGIN CERTIFICATE-----
      YOUR_CA_CERTIFICATE_HERE
      -----END CERTIFICATE-----

llmd:
  namespace: llm-d-inference-scheduling
  modelName: ms-inference-scheduling-llm-d-modelservice
  modelID: "unsloth/Meta-Llama-3.1-8B"
va:
  enabled: true
  accelerator: H100
  sloTpot: 10
  sloTtft: 1000
hpa:
  enabled: true
  maxReplicas: 10
  targetAverageValue: "1"
vllmService:
  enabled: true
  nodePort: 30000
  interval: 15s
  scheme: http  # vLLM emulator runs on HTTP
