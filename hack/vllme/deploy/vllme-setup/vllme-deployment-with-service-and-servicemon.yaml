apiVersion: apps/v1
kind: Deployment
metadata:
  name: vllme-deployment
  namespace: llm-d-sim
spec:
  replicas: 1
  selector:
    matchLabels:
      app: vllme
      llm-d.ai/inferenceServing: "true"
      llm-d.ai/model: ms-sim-llm-d-modelservice
  template:
    metadata:
      labels:
        app: vllme
        llm-d.ai/inferenceServing: "true"
        llm-d.ai/model: ms-sim-llm-d-modelservice
    spec:
      containers:
      - name: vllme
        image: quay.io/infernoautoscaler/vllme:0.2.1
        imagePullPolicy: Always
        env: 
        - name: MODEL_NAME
          value: "default/default" 
        - name: DECODE_TIME
          value: "20"        # In milliseconds, e.g., 50ms per token decode
        - name: PREFILL_TIME
          value: "20"       # In milliseconds, e.g., 100ms for prefill
        - name: MODEL_SIZE
          value: "25000"     # In MB, e.g., 25GB model size
        - name: KVC_PER_TOKEN
          value: "2"       # In MB, e.g., 100MB per token for KV cache
        - name: MAX_SEQ_LEN
          value: "2048"      # Max sequence length
        - name: MEM_SIZE
          value: "80000"     # Total device memory in MB, e.g., 80GB
        - name: AVG_TOKENS
          value: "128"       # Average generated tokens
        - name: TOKENS_DISTRIBUTION
          value: "deterministic"   # "uniform", "normal", "deterministic"
        - name: MAX_BATCH_SIZE
          value: "8"         # Max concurrent requests in a batch 
        - name: REALTIME
          value: "True"      # Boolean: "True" or "False"
        - name: MUTE_PRINT
          value: "False"     # Boolean: "True" or "False"
        ports:
        - containerPort: 80
        resources:
          limits:
            cpu: 500m
            memory: 1Gi
            nvidia.com/gpu: "1"  # Limit to 1 GPU
          requests:
            cpu: 100m
            memory: 500Mi
            nvidia.com/gpu: "1"  # Request 1 GPU
---
apiVersion: v1
kind: Service
metadata:
  name: vllme-service
  namespace: llm-d-sim
  labels:
    app: vllme
spec:
  selector:
    app: vllme  
  ports:
    - name: vllme
      port: 80
      protocol: TCP
      targetPort: 80
      nodePort: 30000
  type: NodePort
---
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: vllme-servicemonitor
  namespace: inferno-autoscaler-monitoring
  labels:
    app: vllme
    release: kube-prometheus-stack
spec:
  selector:
    matchLabels:
      app: vllme
  endpoints:
  - port: vllme
    path: /metrics
    interval: 15s
  namespaceSelector:
    any: true
---
